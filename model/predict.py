import os
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from transformers import BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset, DataLoader
import joblib
import json

# âœ… ì‹¤í–‰ ì¥ì¹˜: í•­ìƒ GPU(MPS) ì‚¬ìš©
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
print(f"âœ… ì‹¤í–‰ ì¥ì¹˜: {device}")

# âœ… Mixed Precision (ë°˜ì •ë°€ë„ ì—°ì‚°) í™œì„±í™” â†’ ì†ë„ ê°œì„ 
scaler = torch.cuda.amp.GradScaler(enabled=(device.type == "mps"))

# âœ… 1. BERT ë¶„ë¥˜ ëª¨ë¸ ì •ì˜
class BERTClassifier(nn.Module):
    def __init__(self, num_classes, dropout=0.3):
        super(BERTClassifier, self).__init__()
        self.bert = BertModel.from_pretrained("bert-base-multilingual-cased")
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        with torch.cuda.amp.autocast(enabled=(device.type == "mps")):  # âœ… Mixed Precision ì ìš©
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            pooled_output = outputs.pooler_output
            return self.fc(self.dropout(pooled_output))


# âœ… 2. PyTorch Dataset ì •ì˜
class WebtoonDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        tokens = self.tokenizer(text, padding="max_length", truncation=True, max_length=self.max_len,
                                return_tensors="pt")

        input_ids = tokens["input_ids"].squeeze(0)  # (1, max_len) â†’ (max_len)
        attention_mask = tokens["attention_mask"].squeeze(0)

        return input_ids, attention_mask, label


# âœ… 3. ì¶”ê°€ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ êµ¬í˜„
def continue_training(df, path, epochs_to_train=10):
    if os.path.exists(path):
        df_new = pd.read_excel(path, engine="openpyxl")
        df = pd.concat([df, df_new], ignore_index=True)  # âœ… ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
        print("âœ… ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ!")

    # âœ… ì €ì¥ ê²½ë¡œ ì„¤ì •
    local_model_dir = os.path.expanduser("~/PycharmProjects/transformer/models")
    os.makedirs(local_model_dir, exist_ok=True)

    label_encoder_path = os.path.join(local_model_dir, "label_encoder.pkl")
    model_path = os.path.join(local_model_dir, "bert_webtoon_classifier.pth")

    tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")

    # âœ… ê¸°ì¡´ ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
    if os.path.exists(label_encoder_path):
        label_encoder = joblib.load(label_encoder_path)
        print(f"âœ… ê¸°ì¡´ ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ: {label_encoder_path}")
    else:
        label_encoder = LabelEncoder()
        print("ğŸš€ ìƒˆë¡œìš´ ë¼ë²¨ ì¸ì½”ë” ìƒì„±")

    df["genre"] = df["genre"].apply(lambda x: eval(x)[0])
    df["genre_label"] = label_encoder.fit_transform(df["genre"])

    # âœ… ë¼ë²¨ ì¸ì½”ë” ì €ì¥
    joblib.dump(label_encoder, label_encoder_path)
    print(f"âœ… ë¼ë²¨ ì¸ì½”ë” ì €ì¥ ì™„ë£Œ: {label_encoder_path}")

    # âœ… ë°ì´í„° ë¶„í• 
    train_texts, test_texts, train_labels, test_labels = train_test_split(
        df["synopsis"].tolist(), df["genre_label"].tolist(), test_size=0.2, random_state=42
    )

    batch_size = 8  # âœ… ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ê¸°ì¡´ 4 â†’ 8)
    accumulation_steps = 2  # âœ… Gradient Accumulation ì¡°ì • (ê¸°ì¡´ 4 â†’ 2)

    train_dataset = WebtoonDataset(train_texts, train_labels, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    num_classes = len(label_encoder.classes_)

    # âœ… ëª¨ë¸ ë¡œë“œ
    model = BERTClassifier(num_classes=num_classes).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs_to_train):
        model.train()
        total_loss = 0
        optimizer.zero_grad()

        for i, (input_ids, attention_mask, labels) in enumerate(train_loader):
            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

            with torch.cuda.amp.autocast(enabled=(device.type == "mps")):  # âœ… Mixed Precision ì ìš©
                outputs = model(input_ids, attention_mask)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()  # âœ… Mixed Precision ìŠ¤ì¼€ì¼ë§

            if (i + 1) % accumulation_steps == 0:  # âœ… Gradient Accumulation ì ìš©
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            total_loss += loss.item()

            # âœ… MPS ìºì‹œ ì •ë¦¬ (ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€) - ì£¼ê¸° ì¤„ì„
            if device.type == "mps" and (i + 1) % 10 == 0:
                torch.mps.empty_cache()

        print(f"Epoch [{epoch + 1}/{epochs_to_train}], Loss: {total_loss / len(train_loader):.4f}")

        # âœ… ëª¨ë¸ ì €ì¥
        torch.save(model.state_dict(), model_path)
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ (Epoch: {epoch + 1})")

    print("ğŸ‰ ì¶”ê°€ í•™ìŠµ ì™„ë£Œ!")


# âœ… ì‹¤í–‰
if __name__ == "__main__":
    text = "synopsis"
    label = "genre"
    fold = "model_data"

    df = pd.read_excel("../data/NAVER-Webtoon_OSMU.xlsx", engine="openpyxl")

    new_data_path = "../data/new_data.xlsx"
    continue_training(df, new_data_path, epochs_to_train=10)
